# self-driving-car

https://drive.google.com/file/d/1F2BKa4mVOc6REwV8jnQpjDics8YdIx3E/view

https://drive.google.com/file/d/1mV2u5F--eTUUX6ZR6o2jAT_o2m3YfdzB/view

Hi Nana, hello everybody,
I agree with some of the comments made this far, but here are some additional thoughts.
There are many steps in building a sound ANN (artificial neural network) model to be used with a set of data. In saying this, what I say is that the specifications of the ANN do depend on your data (i.e., your particular problem you want the ANN to solve).
Nana, it is not clear from your description of your ANN what the activation function is (for each unit), and I don't grasp quite well what the cost function is either. I recommend that you use a sigmoid activation function and the cross-entropy cost function, because this combination allows for avoiding a lot of problems (upper- and lower-boundaries problems, and respectively local minima problem).
Then, start with the simplest ANN architecture, that is a 3-layer network. As for the number of hidden layer units, try use the less possible (start with 5, for instance), and allow for more if the network is unable to learn the problem. If the problem requires very complicated nonlinear separations, it may be necessary that you introduce a 2nd hidden layer, but you need to try first with just 1 hidden layer.
Data normalization (if necessary) should be carried out on the full set of your data at once, before you decide which part of your data will be training data and which part of your data will be cross-validation data.
I suggest that you split the full set of your data into 3 sets: training set (about 1/3 of the data), test set (about 1/3 of the data), and cross-validation set (about 1/3 of the data). Split your data set into these 3 sets RANDOMLY. Train your simplest ANN architecture (i.e., 3-layer ANN, with one hidden layer) on the training set until the RMS error drops to 0.1. If the RMS error gets down to 0.1 then you don't need to add hidden units to the hidden layer, otherwise (e.g., if you're stuck with a RMS error of 0.4) you'll need to add hidden units (or, if the problem is really very complicated, even a 2nd hidden layer). However, you don't want the RMS error to go as low as 0.02, as you mention, as this would (possibly) mean over-learning. Over-learning means your trained ANN is very good on the particular set it was trained on, but that comes at the expense of generalization : generalization will be lousy. An RMS of 0.1 is fine at first. Particularly if you get it after about the same number of epochs whatever the training set "chosen". I mean, you should split the full set of your data into the 3 sets mentioned earlier many times, by "chosing" randomly each time the training exemplars that are assigned to each of the 3 sets.
Now, what is the use of the 3 sets. The training set is obviously used to train the network. The test set is used to test generalization during the training phase (i.e., the ANN is not trained on this set, but it is tested on it during the training). If the error curve on the training set separates itself too much from the error curve on the test set, then it means over-specialization (i.e., instead of coming up with a general solution, the ANN picks up a particular solution that works for the training set but not for the test set). This means you have too many hidden layer units. Use an ANN with less hidden layer units and start over. After some fiddling you'll find an ANN architecture that is both able to solve the problem (i.e., is able to learn up to a RMS error criterion -- of, say, 0.05) and to solve it in a way that is not particular to the training set (i.e., that is general enough to encompass the test set). In practice, when you'll get there, the error curve on the training set and the error curve on the test set will be placed (virtually) on top on of the other.
At this point you want to be sure that this is not the result of a particular choice of the exemplars that were assigned to the training and the test set. A first step is to test with the (up to now) never used cross-validation exemplars. If the error on the cross-validation set is about the same as that on the training and test sets, everything is OK (I like at this point having an additional test, which I describe in my following sentence). To make super-sure everything is OK, at this point I take up again the whole set of data and randomly split it into 3 thirds ( training set (about 1/3 of the data), test set (about 1/3 of the data), and cross-validation set (about 1/3 of the data)) and used the ANN architecture that I retained previously just as described above. If you get the same result as above (i.e., once the ANN has learned, the error on the cross-validation set is about the same as that on the training and test sets) then you've nailed it! Otherwise, look for "not-learning" (the error on the training set does not go down, too few hidden layer units) or over-specialization (see above) and fiddle some more.
One more thing: do not chose by yourself the training set, "chose" its exemplars randomly among those of your whole data set.
Also: do not use a too small part of your whole data set as training exemplars (training set); 1/3 is OK, 1/10 is most often than not NOT OK.
Also: ensure your data set is not non-uniformely noisy, that is, ensure there are no data points that are completely aberrant (this has nothing to do with ANNs, you'll have to check it in the real world (I mean make sure what you'll ultimately feed to your ANN is sensible in our world, to begin with)).
Hope this helps.
Cheers,
SCM
